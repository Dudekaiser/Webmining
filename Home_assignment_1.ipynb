{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 1\n",
    "\n",
    "You should work on the assignement in groups of 3 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to moodle by 16th of May 23:59h. (The deadline is strict)\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawling web pages\n",
    "Consider the wikipedia page on 'World Wide Web'. The task is to crawl the webpages following a breadth-first search (BFS). You need to set the pllimit to 50.\n",
    "You should continue until you have reached a (unique) page count of 500 or have exhausted the queue. For each page you need to obtain the extracts and save them (with page title as name) as separate documents. You need to store these pages in a folder titled 'Docs' (please ensure that the name of the folder is 'Docs'). Also create a list with name 'BFS_page_list' containing the titles of the pages in the BFS order.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/World_Wide_Web#mw-head\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#p-search\n",
      "https://en.wikipedia.org/wiki/WWW_(disambiguation)\n",
      "https://en.wikipedia.org/wiki/Web_(disambiguation)\n",
      "https://en.wikipedia.org/wiki/WorldWideWeb\n",
      "https://en.wikipedia.org/wiki/Internet\n",
      "https://en.wikipedia.org/wiki/File:Web_Page.png\n",
      "https://en.wikipedia.org/wiki/File:Web_Index.svg\n",
      "https://en.wikipedia.org/wiki/Web_index\n",
      "https://en.wikipedia.org/wiki/Information_system\n",
      "https://en.wikipedia.org/wiki/Web_resource\n",
      "https://en.wikipedia.org/wiki/URL\n",
      "https://en.wikipedia.org/wiki/Hypertext\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-1\n",
      "https://en.wikipedia.org/wiki/Software_application\n",
      "https://en.wikipedia.org/wiki/Web_browser\n",
      "https://en.wikipedia.org/wiki/Tim_Berners-Lee\n",
      "https://en.wikipedia.org/wiki/CERN\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-2\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-AHT-3\n",
      "https://en.wikipedia.org/wiki/Information_Age\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-4\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-5\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-6\n",
      "https://en.wikipedia.org/wiki/Web_page\n",
      "https://en.wikipedia.org/wiki/Formatted_text\n",
      "https://en.wikipedia.org/wiki/HTML\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-7\n",
      "https://en.wikipedia.org/wiki/Hyperlink\n",
      "https://en.wikipedia.org/wiki/Web_navigation\n",
      "https://en.wikipedia.org/wiki/Plain_text\n",
      "https://en.wikipedia.org/wiki/Image\n",
      "https://en.wikipedia.org/wiki/Video\n",
      "https://en.wikipedia.org/wiki/Audio_signal\n",
      "https://en.wikipedia.org/wiki/Browser_engine\n",
      "https://en.wikipedia.org/wiki/User_(computing)\n",
      "https://en.wikipedia.org/wiki/Multimedia\n",
      "https://en.wikipedia.org/wiki/Domain_name\n",
      "https://en.wikipedia.org/wiki/Website\n",
      "https://en.wikipedia.org/wiki/Web_server\n",
      "https://en.wikipedia.org/wiki/User-generated_content\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#History\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Function\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#HTML\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Linking\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#WWW_prefix\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Scheme_specifiers\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Web_page\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Static_web_page\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Dynamic_web_pages\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Website\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Web_browser\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Web_server\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#HTTP_(web)_cookie\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Search_engine\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Deep_web\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Web_security\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Privacy\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Standards\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Accessibility\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Internationalisation\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Web_caching\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#See_also\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#References\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#Further_reading\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#External_links\n",
      "https://en.wikipedia.org/w/index.php?title=World_Wide_Web&action=edit&section=1\n",
      "https://en.wikipedia.org/wiki/History_of_the_World_Wide_Web\n",
      "https://en.wikipedia.org/wiki/File:NeXTcube_first_webserver.JPG\n",
      "https://en.wikipedia.org/wiki/NeXT_Computer\n",
      "https://en.wikipedia.org/wiki/File:CERN_web_corridor.jpg\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-8\n",
      "https://en.wikipedia.org/wiki/Global_Internet_usage\n",
      "https://en.wikipedia.org/wiki/Domain_Name_System\n",
      "https://en.wikipedia.org/wiki/Uniform_Resource_Locator\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-9\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-10\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-11\n",
      "https://en.wikipedia.org/wiki/ENQUIRE\n",
      "https://en.wikipedia.org/wiki/Network_address\n",
      "https://en.wikipedia.org/wiki/Hypermedia\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-12\n",
      "https://en.wikipedia.org/wiki/Robert_Cailliau\n",
      "https://en.wikipedia.org/wiki/Client%E2%80%93server_architecture\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-W90-13\n",
      "https://en.wikipedia.org/wiki/HTTP\n",
      "https://en.wikipedia.org/wiki/Wiki\n",
      "https://en.wikipedia.org/wiki/WebDAV\n",
      "https://en.wikipedia.org/wiki/Blog\n",
      "https://en.wikipedia.org/wiki/Web_2.0\n",
      "https://en.wikipedia.org/wiki/RSS\n",
      "https://en.wikipedia.org/wiki/Atom_(standard)\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web#cite_note-14\n",
      "https://en.wikipedia.org/wiki/File:Cern_datacenter.jpg\n",
      "https://en.wikipedia.org/wiki/Data_center\n",
      "https://en.wikipedia.org/wiki/SGML\n",
      "https://en.wikipedia.org/wiki/Dynatext\n",
      "https://en.wikipedia.org/wiki/Institute_for_Research_in_Information_and_Scholarship\n",
      "https://en.wikipedia.org/wiki/Brown_University\n",
      "https://en.wikipedia.org/wiki/HyTime\n",
      "Pages crawled:\n",
      "https://en.wikipedia.org/wiki/World_Wide_Web\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from collections import deque\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import sys\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/World_Wide_Web\"\n",
    "\n",
    "some_params={'action': 'query',\n",
    "            'titles': 'World Wide Web',\n",
    "            'prop': 'links', \n",
    "            'pllimit':'50',  \n",
    "            'format': 'json'}\n",
    "\n",
    "# Create queue\n",
    "queue = deque([])\n",
    "\n",
    "# Maintains list of visited pages\n",
    "visited_list = []\n",
    "\n",
    "\n",
    "# Crawl the page and populate the queue with newly found URLs\n",
    "def crawl(url):\n",
    "    visited_list.append(url)\n",
    "    if len(queue) > 99:\n",
    "        return\n",
    "\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html.read())\n",
    "    urls = soup.findAll(\"a\", href=True)\n",
    "\n",
    "    for i in urls:\n",
    "        flag = 0\n",
    "        # Complete relative URLs and strip trailing slash\n",
    "        complete_url = urljoin(url, i[\"href\"]).rstrip('/')\n",
    "\n",
    "        # Check if the URL already exists in the queue\n",
    "        for j in queue:\n",
    "            if j == complete_url:\n",
    "                flag = 1\n",
    "                break\n",
    "\n",
    "        # If not found in queue\n",
    "        if flag == 0:\n",
    "            if len(queue) > 99:\n",
    "                return\n",
    "            if (visited_list.count(complete_url)) == 0:\n",
    "                queue.append(complete_url)\n",
    "\n",
    "    # Pop one URL from the queue from the left side so that it can be crawled\n",
    "    current = queue.popleft()\n",
    "    # Recursive call to crawl until the queue is populated with 100 URLs\n",
    "    crawl(current)\n",
    "\n",
    "crawl(url)\n",
    "\n",
    "# Print queue\n",
    "for i in queue:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "print(\"Pages crawled:\" )\n",
    "\n",
    "\n",
    "# Print list of visited pages\n",
    "for i in visited_list:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing \n",
    "Remove the stopwords from the documents obtained in the previous step and also perform stemming. You should use the stopword list provided by Python-nltk library. You are required to store the processed documents in a new folder named 'ProcessedDocs'. Name of the pages should be same as the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Calculating relevance score of documents\n",
    "Consider a query 'hypertext web internet' and calculate the similarity scores for all the documents related to the query using latent semantic indexing with SVD and 4-concept space. (Use cosine similarity as the similarity metric). Store the similarity scores in a dictionary named 'LSIscores' with the names of the pages as keys and similarities as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creating network of webpages\n",
    "Create a network of web pages crawled in the first step. The nodes are web pages and a link appears between two pages a and b if a has a link to b in its page. Note that you should only consider the links to web pages that you have crawled and not all the links. Store the adjacency matrix in a variable named 'adjacency\\_mat' with the nodes ordered alphabetically. Also store the number of nodes and edges of the obtained graph in variables 'nodes\\_' and 'edges\\_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dynamic PageRank\n",
    "Consider a random walk setting where the transistion matrix changes over time. At any point of time the probability of a random surfer to jump to a linked page is proportional to the number of previous visits. To start with all the pages are equally likely to be chosen but as the walk continues and the nodes are visited the transition probability changes as proportional to number of previous visits. For example let a page 'a' is linked to pages 'b', 'c' and 'd'. The random surfer currently resides at 'a' and the pages 'b', 'c' and 'd' have already been visited 5, 3 and 2 times respectively. The transition probability would be 0.5, 0.3 and 0.2 respectively. As a new node is viited the probabilities change. The random surfer continues to surf with probability 0.95. Generate 500 random walks and rank the nodes based on the frequency of visit. The random walk should be performed on the network obtained in the previous step.\n",
    "\n",
    "The function should be of the form def dynamicPageRank(graph=G, num_walks=500, telprob=0.8). You should store the results in a dictionary named 'PageRankscores' with keys as page names and number of visits as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Combined ranking\n",
    "Rank the documents based on the relevance score obtained in step 3 as well as the frequency of visits in the previous step (dynamic PageRank). Combine the rankings using Borda Ranking and obtain the top 10 most relevant web pages (documents). You should print the title of the web pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
